 % !TeX spellcheck = en_US
\documentclass[12pt]{article}
\usepackage{amssymb }
\usepackage{amsmath }
\usepackage{graphicx}


%opening
\title{Basic Mathematics for Biologists}
\author{Alberto Marin Sanguino}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Ordinary differential equations}

A differential equation is an equality involving one or more unknowns ($x$,$y$,$z$,\dots) and their derivatives. Since our unknowns have derivatives, they are not just numbers like in regular -- a.k.a. algebraic -- equations. Our unknowns are functions or, more accurately, families of functions. We can make this explicit by writing our unknowns as $x(t)$,$y(t)$,$z(t)$,\dots thus sacrificing brevity for the sake of clarity. In the text that follows, very little effort will be made to mantain consistency between these two notations and we will wantonly switch between both. Live with it.

Among all differential equations, we will be interested in the simpler type. Ordinary differential equations (ODEs) are those that do not involve partial derivatives. Our functions will only have one independent variable that almost always will be time. Moreover, we will start with linear equations.

Linear equations are those where different unknowns and their derivatives are only combined linearly. Some examples of linear equations are:

\begin{align}
	\frac{dx}{dt} \: = \: &  a \, x	\\
	\frac{dx}{dt} + \frac{dy}{dt}\: = \: &  0 \\
	\frac{dx}{dt} \: = \: &  a \, x + b \, y
\end{align}

while the following equations are not linear:

\begin{align}
	\frac{dx}{dt} \: = \: &  x^2	\\
	\frac{dx}{dt} + \frac{dy}{dt}\: = \: &  x \, y \\
	\frac{dx}{dt} \: = \: &  \sin x 
\end{align}

Keep in mind that the linearity rrequirement only applies to the unknowns and not to the independent variable. The following equations are all linear


\begin{align}
	\frac{dx}{dt} \: = \: &  t^2 \, x	\\
	\frac{dx}{dt} + \frac{dy}{dt}\: = \: &  \sin t \\
	\frac{dx}{dt} \: = \: &  t \, x + t^2 \, y
\end{align}

In these equations, the coefficients that multiply our unknowns just happen to be functions of time. A linear equation can have coefficients that depend on time and also have time dependent independent terms. In general, we will not concern ourselves with such systems. Lets start with the simplest possible case, which also happens to describe a fundamental biological process: exponential grow.

\begin{equation}
	\label{odexp}
	\frac{dx}{dt} = \mu \, x
\end{equation}

Which can be read as: ``the rate of increase of a population $x$ is proportional to its own size.'' This equation is easy to solve because we can separate the variables:


\begin{equation}
	\frac{dx}{x} = \mu \, dt
\end{equation}

and integrate

\begin{equation}
	\int \frac{dx}{x} = \mu \, \int  dt
\end{equation}

\begin{equation}
	\ln{x} = \mu \, t + C
\end{equation}

Since $C$ is an arbitrary constant, we can define it in terms of another constant $C= \ln{k}$ so:

\begin{equation}
	\ln{x} - \ln{k}  = \mu \, t 
\end{equation}

Applying the well known properties of logarithms listed in appendix \ref{logprop}

\begin{equation}
	\ln{\frac{x}{k}}   = \mu \, t 
\end{equation}

exponentiating both sides and rearranging

\begin{equation}
	x   = k \, e^{\mu \, t}
\end{equation}

This is the \emph{general solution} of the differential equation. For every possible value of $k$ there is a \emph{particular solution} of the equation.

We are not normally interested in general solutions. General solutions are for mathematicians. We want the particular solution that fits some \emph{initial condition}, $x(0)$, often written $x_0$. For instance, we have a bacterial culture that starts with an optical density of 0.01. How do we find the right value for k?


\begin{equation}
x(0) = 0.01 \Rightarrow	 k \, e^{\mu \, 0} = 0.01 \Rightarrow	k=0.01 
\end{equation}

In general, we use this equation as:

\begin{equation}
	x   = x_0 \, e^{\mu \, t}
\end{equation}

\section{Solving linear equations with more than one variable}

The concepts above can easily be extended to more variables:

\begin{align}
	\label{odenvar}
	\frac{dx_1}{dt} =& a_{1,1} \, x_1 + a_{1,2} \, x_2 \nonumber\\
	\frac{dx_2}{dt} =& a_{2,1} \, x_1 + a_{2,2} \, x_2
\end{align}

when dealing with several variables, it helps a lot to use vectors and matrices.

\begin{equation}
	\frac{d}{dt} \begin{pmatrix} x_1\\ x_2 \end{pmatrix} = \begin{pmatrix} a_{1,1} & a_{1,2}\\ a_{2,1} & a_{2,2} \end{pmatrix} \begin{pmatrix} x_1\\ x_2 \end{pmatrix}
\end{equation}

Thus, we can write any equation with n variables as:

\begin{equation}
	\label{odenvar_mat}
	\frac{d\mathbf{x}}{dt}  = \mathbf{A} \, \mathbf{x}
\end{equation}

Note that vectors and matrices are written in boldface.

Since the solution of the equation for one variable was an exponential, lets make the hypothesis that an exponential can also be a solution for this case. We assume

\begin{equation}
	\mathbf{x}= \mathbf{v} \, e^{\lambda \, t}
\end{equation}

Since $\mathbf{x}$ is a vector now, we multiply our exponential times a  vector of constants $\mathbf{v}$. We can test if this is a valid solution by substituting it into the equation, which gives:

\begin{equation}
	\frac{d}{dt} \left( \mathbf{v} \, e^{\lambda \, t} \right)  = \mathbf{A} \, \mathbf{v} \, e^{\lambda \, t} 
\end{equation}

On the left hand side, the vector of  constants can be taken out of the derivative so:

\begin{equation}
	\mathbf{v} \, \frac{d}{dt} \left(  e^{\lambda \, t} \right)  = \mathbf{A} \, \mathbf{v} \, e^{\lambda \, t} 
\end{equation}
 
 and
 
 \begin{equation}
 	\mathbf{v}   \lambda \, e^{\lambda \, t}   = \mathbf{A} \, \mathbf{v} \, e^{\lambda \, t} 
 \end{equation}
 
canceling out the exponentials:

\begin{equation}
\label{eigendefinition}
\mathbf{A} \, \mathbf{v}  = \lambda  \,	\mathbf{v}   
\end{equation}
 
So $\mathbf{x}=\mathbf{v} \, e^{\lambda \, t}$ will be a solution of the equation as long as $\lambda$ and $\mathbf{v}$ satisfy equation (\ref{eigendefinition}).
\section{Eigenvalues and eigenvectors}

A matrix $\mathbf{A}$ with size $n \times n$ has n vectors $\mathbf{v_i}$ for $i = 1 \dots n$ called eigenvectors, and each eigenvector has an eigenvalue ($\lambda_i$) such that every pair of eigenvector and eigenvalue satisfy equation (\ref{eigendefinition}).
	
In order to understand the meaning of this and why eigenvector and eigenvalues are extremely important in applied mathematics, we have to think of the matrix product as an operation that transforms one vector into another. For instance, lets take the matrix
\begin{equation}
	\mathbf{A}  = \begin{pmatrix} -1 & 0\\ 0 & 2\end{pmatrix} 
\end{equation}



If we apply this operation to the vector (-1,1) we obtain:

\begin{equation}
	\begin{pmatrix}  5\\ 3  \end{pmatrix}  = \begin{pmatrix} -1 & 0\\ 0 & 2 \end{pmatrix} \begin{pmatrix}  1\\ 3  \end{pmatrix}
\end{equation}

So out matrix transforms vector (1,3) into vector (5,3).

%WORKING HERE


\section{General solutions for linear equations}

So putting together all the eigenvecotrs of the system and the corresponding eigenvalues, we get the general solution of the system of equations:


\begin{equation}
	\label{odenvar_mat_sol}
	\mathbf{x}= c_1 \, \mathbf{v_1} \, e^{\lambda_1 \, t} + c_2 \, \mathbf{v_2} \, e^{\lambda_2 \, t} +  \dots + c_n \, \mathbf{v_n} \, e^{\lambda_n \, t}
\end{equation}

Where $c_1 \dots c_n$  are arbitrary constants, the ones that can be used to adjust initial values. Each vector $\mathbf{v_i}$ and its corresponding constant $\lambda_i$ are called eigenvector and eigenvalue respectively.


\section{Differential equations of higher order}

\section{Plotting solutions}


\appendix
\section{Some properties of logarithms}
\label{logprop}
\begin{equation}
 \log{(a \, b)} =	\log{a} + \log{b} 
\end{equation}

\begin{equation}
	\log{(a / b)} =	\log{a} - \log{b} 
\end{equation}

\begin{equation}
	\log{x^a} =a \, \log{x} 
\end{equation}












\end{document}
