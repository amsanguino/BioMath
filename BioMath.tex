 % !TeX spellcheck = en_US
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Block from the:
% Tufte-Style Book (Documentation Template)
% LaTeX Template
% Version 1.0 (5/1/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% The Tufte-LaTeX Developers (tufte-latex.googlecode.com)
%
% License:
% Apache License (Version 2.0)
%
% IMPORTANT NOTE:
% In addition to running BibTeX to compile the reference list from the .bib
% file, you will need to run MakeIndex to compile the index at the end of the
% document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{tufte-book} % Use the tufte-book class which in turn uses the tufte-common class

\hypersetup{colorlinks} % Comment this line if you don't wish to have colored links

\usepackage{microtype} % Improves character and word spacing

\usepackage{lipsum} % Inserts dummy text

\usepackage{booktabs} % Better horizontal rules in tables

\usepackage{graphicx} % Needed to insert images into the document
\graphicspath{{figures/}} % Sets the default location of pictures
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio} % Improves figure scaling

\usepackage{fancyvrb} % Allows customization of verbatim environments
\fvset{fontsize=\normalsize} % The font size of all verbatim text can be changed here

\newcommand{\hangp}[1]{\makebox[0pt][r]{(}#1\makebox[0pt][l]{)}} % New command to create parentheses around text in tables which take up no horizontal space - this improves column spacing
\newcommand{\hangstar}{\makebox[0pt][l]{*}} % New command to create asterisks in tables which take up no horizontal space - this improves column spacing

\usepackage{xspace} % Used for printing a trailing space better than using a tilde (~) using the \xspace command

\newcommand{\monthyear}{\ifcase\month\or January\or February\or March\or April\or May\or June\or July\or August\or September\or October\or November\or December\fi\space\number\year} % A command to print the current month and year

\newcommand{\openepigraph}[2]{ % This block sets up a command for printing an epigraph with 2 arguments - the quote and the author
	\begin{fullwidth}
		\sffamily\large
		\begin{doublespace}
			\noindent\allcaps{#1}\\ % The quote
			\noindent\allcaps{#2} % The author
		\end{doublespace}
	\end{fullwidth}
}

\newcommand{\blankpage}{\newpage\hbox{}\thispagestyle{empty}\newpage} % Command to insert a blank page

\usepackage{units} % Used for printing standard units

\newcommand{\hlred}[1]{\textcolor{Maroon}{#1}} % Print text in maroon
\newcommand{\hangleft}[1]{\makebox[0pt][r]{#1}} % Used for printing commands in the index, moves the slash left so the command name aligns with the rest of the text in the index 
\newcommand{\hairsp}{\hspace{1pt}} % Command to print a very short space
\newcommand{\ie}{\textit{i.\hairsp{}e.}\xspace} % Command to print i.e.
\newcommand{\eg}{\textit{e.\hairsp{}g.}\xspace} % Command to print e.g.
\newcommand{\na}{\quad--} % Used in tables for N/A cells
\newcommand{\measure}[3]{#1/#2$\times$\unit[#3]{pc}} % Typesets the font size, leading, and measure in the form of: 10/12x26 pc.
\newcommand{\tuftebs}{\symbol{'134}} % Command to print a backslash in tt type in OT1/T1

\providecommand{\XeLaTeX}{X\lower.5ex\hbox{\kern-0.15em\reflectbox{E}}\kern-0.1em\LaTeX}
\newcommand{\tXeLaTeX}{\XeLaTeX\index{XeLaTeX@\protect\XeLaTeX}} % Command to print the XeLaTeX logo while simultaneously adding the position to the index

\newcommand{\doccmdnoindex}[2][]{\texttt{\tuftebs#2}} % Command to print a command in texttt with a backslash of tt type without inserting the command into the index

\newcommand{\doccmddef}[2][]{\hlred{\texttt{\tuftebs#2}}\label{cmd:#2}\ifthenelse{\isempty{#1}} % Command to define a command in red and add it to the index
	{ % If no package is specified, add the command to the index
		\index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% Command name
	}
	{ % If a package is also specified as a second argument, add the command and package to the index
		\index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% Command name
		\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% Package name
}}

\newcommand{\doccmd}[2][]{% Command to define a command and add it to the index
	\texttt{\tuftebs#2}%
	\ifthenelse{\isempty{#1}}% If no package is specified, add the command to the index
	{%
		\index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2}}% Command name
	}
	{%
		\index{#2 command@\protect\hangleft{\texttt{\tuftebs}}\texttt{#2} (\texttt{#1} package)}% Command name
		\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}% Package name
}}

% A bunch of new commands to print commands, arguments, environments, classes, etc within the text using the correct formatting
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}
\newenvironment{docspec}{\begin{quotation}\ttfamily\parskip0pt\parindent0pt\ignorespaces}{\end{quotation}}
\newcommand{\docenv}[1]{\texttt{#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}
\newcommand{\docenvdef}[1]{\hlred{\texttt{#1}}\label{env:#1}\index{#1 environment@\texttt{#1} environment}\index{environments!#1@\texttt{#1}}}
\newcommand{\docpkg}[1]{\texttt{#1}\index{#1 package@\texttt{#1} package}\index{packages!#1@\texttt{#1}}}
\newcommand{\doccls}[1]{\texttt{#1}}
\newcommand{\docclsopt}[1]{\texttt{#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}
\newcommand{\docclsoptdef}[1]{\hlred{\texttt{#1}}\label{clsopt:#1}\index{#1 class option@\texttt{#1} class option}\index{class options!#1@\texttt{#1}}}
\newcommand{\docmsg}[2]{\bigskip\begin{fullwidth}\noindent\ttfamily#1\end{fullwidth}\medskip\par\noindent#2}
\newcommand{\docfilehook}[2]{\texttt{#1}\index{file hooks!#2}\index{#1@\texttt{#1}}}
\newcommand{\doccounter}[1]{\texttt{#1}\index{#1 counter@\texttt{#1} counter}}

\usepackage{makeidx} % Used to generate the index
\makeindex % Generate the index which is printed at the end of the document

% This block contains a number of shortcuts used throughout the book
\newcommand{\vdqi}{\textit{VDQI}\xspace}
\newcommand{\ei}{\textit{EI}\xspace}
\newcommand{\ve}{\textit{VE}\xspace}
\newcommand{\be}{\textit{BE}\xspace}
\newcommand{\VDQI}{\textit{The Visual Display of Quantitative Information}\xspace}
\newcommand{\EI}{\textit{Envisioning Information}\xspace}
\newcommand{\VE}{\textit{Visual Explanations}\xspace}
\newcommand{\BE}{\textit{Beautiful Evidence}\xspace}
\newcommand{\TL}{Tufte-\LaTeX\xspace}

%----------------------------------------------------------------------------------------
%	BOOK META-INFORMATION
%----------------------------------------------------------------------------------------

%\title{A Tufte-Style Book\thanks{Thanks to Edward R.~Tufte for his inspiration.}} % Title of the book

%\author[The Tufte-LaTeX Developers]{The Tufte-LaTeX\ Developers} % Author

%\publisher{Publisher of This Book} % Publisher
%\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{placeins}
%\usepackage{graphicx}
%\usepackage[a4paper, margin=2.5cm]{geometry}

%opening
\title{Systems Biology,\\ an introduction}
\author{Alberto Marin Sanguino}
%\publisher{Publisher of This Book} % Publisher

\begin{document}
\setcounter{tocdepth}{2}
\frontmatter
\maketitle
\tableofcontents

\chapter{Introduction}
\section{Why systems?}

Biology has traditionally been the science without mathematics, a place where the non-mathematically oriented  could develop  scientific careers far from the trials and tribulations of linear algebra, differential calculus and complex analysis. This way of thinking remained dominant in the field until the end of the twentieth century and the sequencing of the human genome, when biologists found themselves drowning in data. Genomic sequences provided a list of all the relevant parts needed to build and operate a cell and, by extension, a full living being. But connecting all those parts and the relevant regulatory signals, also present in the genome, proved to be impossible.\begin{itemize}
	\item The sheer number of components, in the tens of thousands, were very difficult to represent using conventional means, such as diagrams, lists or sketches in a blackboard.
	\item Acting on a component that is part of a network initiates the propagation of multiple signals criss-crossing one another and doubling back onto themselves in ways that are very difficult to follow.
	\item Since the interaction between all the components is quantitative, the same kind of action (inhibiting a step of a pathway) can lead to very different outcomes depending on the intensity of the stimulus.
\end{itemize}    

The combination of these three effects often results counter-intuitive behaviors.

Fortunately for modern biology, these problems had already been encountered in the domain of engineering. The study of machines formed by many different components into a network of complex interactions started in the years following world war II and continued with great intensity in the following decades. The discipline that resulted from this was called systems theory, and its incredible success fueled an era of technological innovation in fields as diverse as mechanics, electronics and chemical engineering.

At the heart of systems Theory is the definition of a system as an ensemble of components connected in a certain manner and limited by a boundary. Signals and materials can cross the boundary into the system -- inputs -- or out of it -- outputs.

Social and economic systems are  rich sources of surprising behaviors. Lets assume I replace all the light bulbs in my home with power saving leds that consume half as much electricity. With such a measure, I will  lower my immediate electricity consumption and save money. In light of these effects, which are short-term and local, I might be temped to think that I am contributing to an overall reduction in energy consumption in my community. Experience, however, shows the opposite. The development and adoption of machines that are more efficient tends to increase the overall consumption in the long run. My modest action changing light bulbs at home, modifies a very intricate system and signals start rippling through it. A reduction in electricity demand of homes, frees kilowatts that can be sent elsewhere. The utility company can sell this surplus electricity at a lower price for industrial purposes. The energy will be used anyway because there is a bigger system beyond the boundaries of my neighborhood -- power plants, factories and much more. This bigger system, has compensated my reduction in power consumption with an equal increase somewhere else. Moreover, producing energy has an energy cost so the energy that has gone to industrial purposes can be redirected to oil extraction or the construction of new solar panels. This, together with the invention of more efficient engines and machinery lowers the cost of oil extraction, which, in turn enables more energy production at cheaper prices that incentivate consumers to use more energy. In the long term, I might end up  buying new appliances that consume as much electricity as I saved before, maybe even more! So the long term effects of my action end up contradicting my initial intention. This phenomenom is called the Jevons paradox and, if we are surprised by it is due to our innate tendency to think both local and short term. If I place the boundary of my system along the walls of my home, I am not taking into account the effects of my actions on other actors, like my neighbors or the appliance company. These effects, will eventually double back on me in the long term through things such as lower electricity prices, power discounts and fancy new appliances.

Just as social systems show surprising behaviors like the Jevons paradox, so too happens with ecological, physiological and biochemical systems. Medical treatments and their side effects are a good example of paradoxical  unintended consequences. Cortisol is a common treatment used to prevent strong inflammatory or immune responses, and patients undergoing such treatment exhibit a symptom commonly known as ``moon face''. The face of the patient acquires a rounder than usual form due to the accumulation of fat on the cheeks and around the throat. The cause for this effect is, surprisingly, that cortisol boosts metabolism and burns calories. The sudden consumption of energy in the form of carbohydrates triggers a response to store more energy in the form of fat-tissue. What are the factors that favor this side effect? Is everybody equally vulnerable to it? Will any metabolic boost result in fat accumulation? Maybe a similar drug that burns less calories will not trigger fat accumulation, or maybe a drug that burns more, will also burn the fat \dots  and why the face?  In order to answer any of these questions, we have to see how all these interconnected mechanisms act on one another forming a network and how those actions propagate through it. In other words, we need systems theory.


\section{Systems biology}

There is no question that complicated networks of interacting components have to be carefully studied in order to understand how they work. Any biologist has spent a significant amount of time studying such intricate networks and memorizing carefully drawn diagrams of metabolic fluxes, regulatory interactions, gene expression, hormone effects, immunologic responses or food-webs. Even though this memorization effort is clearly necessary, we have just seen why it is not sufficient. So as long as we limit our approach to elucidate, draw and memorize these complex networks, we are just deluding ourselves into  believing we know something that we do not.

Instead of going into full denial, biologists can take advantage of the tools developed in other fields. Engineers have developed precise tools for drawing diagrams, making calculations and following signal propagation through complex networks. Many such tools have already been translated to biological systems, creating new fields such as bioinformatics, computational neuro-science or systems biology. The emergence of these new fields has resulted in a much better understanding of complex biological problems such as biochemical and genetic regulation, cognitive processes and mental illness, but it has also resulted in productive exchanges for other sciences such as artificial neural networks, which are at the center of recent developments in artificial intelligence.

If we want to dive into the field of systems biology, we will have to start assimilating some important concepts: 
\begin{itemize}
	\item A \textbf{system} is a set of parts that work together and can be defined within a \textbf{boundary}. If a system can exchange material, energy or information with the outside through the boundary, it is said to be open. If the boundary does not let matter pass through it, the system is said to be closed. When the boundary is so tight that neither matter nor energy may pass through it, we say the system is isolated.
	\item In order to predict the behavior of a system, we often need mathematics. In order to do so, we define a \textbf{mathematical model} as any mathematical representation of our system. A model does not need to be perfect. Some models only capture part of the system behavior and some models give only  approximate results.
	% Add variables, maybe inputs and outputs too ...
	\item When we use a model to make predictions about the behavior of a system, we are making a \textbf{simulation}. Simulations can be done with paper and pencil for very simple models, but they often must be done using a computer.
\end{itemize}

% diagrams I/O example
\section{The mathematics of growth, evolution and change}
Growth is one of the main characteristics of biological systems. All living beings grow at least during a part of their life cycle with most keeping sustained growth through it all. During biological growth, some property of the system -- size, number of individuals, biomass, radius of a tumor \dots - increases with time. Lets call this property, $x$. We can indicate that x changes with time by writing it as a function $x(t)$ or as a series $x_t = x_1, x_2, x_3,\dots$ with $x_i$ being the value of x after i time units.

The simplest problems to analyze mathematically are those that happen at discrete intervals in time. For instance, if a population of microbes duplicates every hour, then the population at time $t+1$ will be twice the population at time $t$.

\begin{equation}
	\label{geomgrowth_example}
	x_{t+1} = 2 \, x_t
\end{equation}

so given the population at the initial time,  $x_0$, then $x_1=2\, x_0$, $x_2=2\, x_1$ and so on. We can calculate the population at time $t$ using the simple formula:

\begin{equation}
	x_{t} = 2^t \, x_0
\end{equation}

This formula can be considered our first mathematical model. We can use it to make a simulation for a particular case. Lets say we start with a single cell. A simulation of our model for the next five generations will be:

\begin{equation}
	x_{t} =1 ,\, 2 ,\,4 ,\,8 ,\,16,\dots
\end{equation}

this is so simple that we hardly need any math!

Once we made a simulation, it is often helpful to show a visual representation. This can be done in many different ways, the two most common are shown in figure \ref{fig:tcourse}. 


\begin{figure}
	\begin{center}
		\includegraphics[width=0.46\textwidth]{time_course}
		\includegraphics[width=0.46\textwidth]{phase_space}
	\end{center}
	\caption{Dynamics of a population of microbes. The graph on the left shows the time course, $x$ vs $t$ while the graph on the right shows the one-dimensional phase space (a line) where a series of points show the successive values of x as dots. The points are connected by arrows to show the progression. The trajectory followed by a system in phase-space is called an orbit.}
	\label{fig:tcourse}
\end{figure}

 We could just plot the results as the state of our system, $x$, against time, we would call that a time course. For more complex systems, we may prefer to show the state of the system as point in space. For instance, our first mathematical model has a single variable so any  state can be represented as a point on the real line.

\chapter{Discrete dynamical systems}

The simple model we saw in the previous section is an example of discrete dynamical systems. Now we move on to a slightly more complicated case. The radius of a tumor increases at a rate of  15 \% every hour, what is the doubling time of the tumor? The general form of equation \ref{geomgrowth_example} for a growth rate $r$ is:

\begin{equation}
\label{geomgrowth_general}
x_{t+1} = x_t  + r \, x_t =(1 + r) \, x_t
\end{equation}

and so

\begin{equation}
\label{geomgrowth_general2}
x_{t} = (1 + r)^{t} \, x_0
\end{equation}


this is called \textbf{geometric growth} with $r=0.15$ for our tumor. 



Biologists often describe growth using its doubling time, $t_d$,  which is the time needed for the system to duplicate its size. So if our tumor starts with size $x_0$, it will become double after a time $t_d$:

\begin{equation}
2\, x_{0} = (1 + r)^{t_d} \, x_0
\end{equation}

where $x_0$ cancels out. Now we can take  logarithms and rearrange:

\begin{equation}
t_d = \frac{\log 2}{\log (1 + r) }
\end{equation}

So our doubling time for $r=0.15$ is 1.74 hours, about an hour and 45 min.

\section{Dealing with many things at once: vectors and matrices}

The  individuals in a population are not always identical. Most of the time, biological populations have many different types of individuals. Cells differentiate into types, animals grow in different stages, microbes can form spores \dots Fortunately, we have the mathematical tools to reflect that diversity. Consider a population of parasites that reproduce through eggs. Every week, some eggs hatch and produce new parasites that in time will produce more eggs. We can easily capture the idea of our population having two different individuals: eggs, $x_1$ and animals, $x_2$. When we want to refer to all individuals of the species, regardless of their type, we will use vector $\mathbf{x}$.

\begin{equation}
\mathbf{x} =\left( \begin{array}{c} x_1 \\  x_2 \end{array} \right)
\end{equation}

A vector of dimension $n$ is just a collection of $n$ ``ordinary'' numbers which, by the way, are called scalars. So the total population is represented by vector  $\mathbf{x}$, which is composed of $x_1$ eggs and $x_2$ animals. We write vectors and matrices (see below) in \textbf{boldface}.


When we study the dynamics of this population, we will see why it was so important to keep the two types separate. The number of eggs produced each generation does not depend on the number of eggs present in it. Eggs are laid by the animals, so the more animals there are, the more eggs are produced:

\begin{equation}
x_1 = f_{1} \,  x_2 
\end{equation}

where $f_1$ is the fecundity of the animals ($x_2$). The production of new animals is also dependent of the fraction of eggs that survive the incubation period so for a survival ratio $s_1$:

\begin{equation}
	x_2 = s_{1} \,  x_1 
\end{equation}

In this simple case, we can just work with the two equations, but as the complexity of the population increases, we have to find a more efficient method to handle the complex interactions between all the groups. Lets assume we are studying a population of mice with three different age groups: young  mice up to one year old, $n_1$, can reproduce at very low rates, mice between 1 and 1.5 years old, $n_2$, are adults in their prime and old mice up to two years old, $n_3$, have more fragile health, which leads to lower fecundity. We can summarize this information making a drawing, as shown in figure \ref{fig:mice}. Each age group is represented by a circle, and each process changing the population, by an arrow. The starting point of an arrow shows which group determines the speed of the process while the  destination of the arrow shows which group is affected by it.

\begin{marginfigure}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{Mouse_pop_graph}
	\end{center}
	\caption{Dynamics of a population of mice. Circles contain the three age groups that are connected by arrows showing how one population affects the growth of another. Symbols on top of each arrow show the coefficient that governs the interaction. For instance, $f_3$ is the fecundity of old mice, $n_3$, while $s_1$ is the fraction of young mice that survive to adulthood.}
	\label{fig:mice}
\end{marginfigure}
 
The figures show clearly that the increase in young mice, $n_1$, for each generation will depend on the fecundities of all three age groups, while the growth of old mice $n_3$, only depends on the survival of adult mice $n_2$. The arrows clearly show which population affects which but, how do we write our equations? Fortunately, mathematics has a simple tool to transform drawings into numbers: matrices.

A matrix is just a square of numbers that connects elements represented in its columns to elements represented in its rows. Since our drawing connects the three elements $n_1$, $n_2$, $n_2$ to one another, we will have a row and a column for each. Lets start making such a square and fill it with zeros:

\begin{center}
\begin{tabular}{c|ccc}
	& $n_1$ & $n_2$ & $n_3$ \\
	\hline 
	$n_1$ &0	&0	&0	\\
	$n_2$ &0	&0	&0	\\
	$n_3$ &0	&0	&0	\\
\end{tabular}
\end{center}

And now, we will place a number for each arrow in the drawing. The starting point of the arrow will be the column where we place the number and the ending point of the arrow will be the row. Our table will look like this.

\begin{center}
	\begin{tabular}{c|ccc}
		& $n_1$ & $n_2$ & $n_3$ \\
		\hline
		$n_1$ &$f_1$	&$f_2$	&$f_3$	\\
		$n_2$ &$s_1$	&0	&0	\\
		$n_3$ &0	&$s_2$	&0	\\
	\end{tabular}
\end{center}

So the fecundity $f_3$ connects $n_3$ (column) to $n_1$ (row) and so on. We do not Normally use the titles for row and columns, so we would write our matrix as:

\begin{equation}
\mathbf{L} =	\begin{pmatrix}
		f_1	&f_2	&f_3	\\
		s_1	&0	&0	\\
		0	&s_2	&0
	\end{pmatrix}
\end{equation}

Now it is very easy to get the equations, since vectors and matrices have their own mathematical operations. The transition from one vector to another ( $n_t \mapsto n_{t+1}$ ) Is obtained by multiplying the initial vector:

\begin{equation}
	\mathbf{n_{t+1}} =\mathbf{L} \cdot \mathbf{n_t}
	\label{fig:matequationL}
\end{equation}

The matrix $\mathbf{L}$ is commonly known as the Leslie matrix to honor Patrick H. Leslie, the ecologist who proposed it. If we now apply the product between vectors and matrices as we learned in high school, we will see that:

\begin{equation}
	 	\begin{pmatrix}
		f_1	&f_2	&f_3	\\
		s_1	&0	&0	\\
		0	&s_2	&0
	\end{pmatrix} \cdot \begin{pmatrix} n_{1,t}\\ n_{2,t} \\ n_{3,t}\end{pmatrix} = \begin{pmatrix} f_1 \,n_{1,t} + f_2 \,n_{2,t} + f_3 \,n_{3,t}\\ s_1 \, n_{1,t} \\ s_2 \, n_{2,t}\end{pmatrix}
\end{equation}

and we can apply our growth equation as many times as we want:

\begin{equation}
	\mathbf{n_{2}} =\mathbf{L} \cdot \mathbf{n_1} = \mathbf{L} \cdot \mathbf{L} \cdot \mathbf{n_0} 
\end{equation}

and for $k$ generations:

\begin{equation}
	\mathbf{n_k} =\mathbf{L}^k \cdot \mathbf{n_0}
\end{equation}

where $\mathbf{L}^k$ mans we multiply the matrix by itself $k$ times.

Equation \ref{fig:matequationL} shows a very useful fact.  By multiplying a vector times a matrix, we \emph{transform} this vector into another. For this reason, we often talk about a matrix as a \textbf{transformation}.  Here we use matrix $\mathbf{L}$ it to transform the current population into the population of the next generation. When we represent anything by a vector -- e.g the direction and intensity of a ray of light -- we can represent changes in ts state by a matrix multiplication --e.g. the direction and intensity of the light after passing through liquid medium. This capacity of abstraction is what makes mathematics so powerful in science and engineering. Solving a problem once in a field like physics means solving hundreds of problems in other fields.

Back to our population of animals, we may wonder how it is going to change in the future.  Is the age distribution of the population going to stabilize in the long run or will it keep changing forever? This kind of questions are also very relevant for economists and social scientists. Will the population of developed countries continue to get older? Or will the aging stop at some point? The solution to this question is important since all our pensions depend on it. Surprisingly enough, the answer was already found in the 18$^{th}$ century by Leonard Euler while he was working on a completely different problem: the rotation of rigid bodies.

\section{Eigenvalues and eigenvectors}

Back to our mice example, look at the behavior of our population along several generations in figure~\ref{fig:Lpop}.

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{pop_evol}
\includegraphics[width=\linewidth]{tot_pop_evol}		
\end{center}
\caption{The upper graph shows the population structure as a bar graph for nine generations. The figure below shows the total population for each generation (the sum of all three age groups). As the total population increases, it is obvious that the vector is different each generation, but the proportion of the three age groups, seems to be very similar for each generation.}
\label{fig:Lpop}
\end{figure*}




The population keeps growing, so the vectors keep changing, but looking at their shape, it looks like it is well preserved from one generation to the next. There are always more young mice than adults and more adults than seniors. Does this mean the population will reach a stable age structure?

Imagine that the vectors of the population for several consecutive generations are as follows:

\begin{equation}
	n_0 = \begin{pmatrix} 3\\ 2 \\ 1\end{pmatrix}, 
	n_1 = \begin{pmatrix} 6\\ 4 \\ 2\end{pmatrix}, 
	n_2 = \begin{pmatrix} 12\\ 8 \\ 4\end{pmatrix}
\end{equation}

It is easy to see that $\mathbf{n_1} = 2 \cdot \mathbf{n_0}$  and $\mathbf{n_2} = 2 \cdot \mathbf{n_1}$. In other words, the matrix preserves the ``shape'' of the vector and just increases its size.

\begin{equation}
 \begin{pmatrix} 6\\ 4 \\ 2\end{pmatrix} =  2 \cdot\begin{pmatrix} 3\\ 2 \\ 1\end{pmatrix} 
 \label{eigenvect_in_action}
\end{equation}

This means our population will keep growing -- in this case, it will double its size every generation -- but the age structure will remain stable with three young mice for every two adults and one senior. In fact, we could have made this prediction just looking at the matrix $\mathbf{L}$. Every matrix has a few vectors that keep their direction when transformed by it. They may become longer or shorter, because they are multiplied by a certain scalar. This property is so important in science and engineering that it has a formal definition we should learn. Given matrix $\mathbf{A}$, we say $\mathbf{v}$ is an eigenvector of  $\mathbf{A}$ if:
 
\begin{equation}
	\label{eigendefinition}
	\mathbf{A} \, \mathbf{v}  = \lambda  \,	\mathbf{v}   
\end{equation}

The number $\lambda$ is a scalar associated to that particular eigenvector and it is called an eigenvalue. In the example shown in equation \ref{eigenvect_in_action}, $(3,2,1)$ would be an eigenvector and $2$ would be its corresponding eigenvalue.

Every matrix of dimensions $n \times n$ has exactly $n$ eigenvectors and $n$ eigenvalues. Eigenvectors can be understood as the natural "axes" to study the transformation. If our coordinates are aligned with the eigenvectors, we can understand any transformation as just ``stretching'' or ``shrinking'' the vector in each direction by the corresponding eigenvalue. For instance, the matrix

\begin{equation}
	\mathbf{A}=\begin{pmatrix} 0.5 & 0\\ 0 &  2 \end{pmatrix} 
\end{equation}

has dimension $2 \times 2$, so it has two eigenvectors with their corresponding eigenvalues. one eigenvector is $\mathbf{v_1}=\begin{pmatrix}  0\\ 1 \end{pmatrix} $,  -- that goes along the y-axis -- and its corresponding eigenvalue is $\lambda_1 = 2$. multiplying the matrix times $\mathbf{v_1}$, proves it is indeed an eigenvector with eigenvalue 2:

\begin{equation}
	\begin{pmatrix} 0.5 & 0\\ 0 &  2 \end{pmatrix} \cdot \begin{pmatrix}  0\\ 1 \end{pmatrix} = \begin{pmatrix}  0\\ 2 \end{pmatrix}= 2 \, \begin{pmatrix}  0\\ 1 \end{pmatrix}
\end{equation}

The same happens with the second eigenvector $\mathbf{v_2}=\begin{pmatrix}  1\\ 0 \end{pmatrix}$ -- on the x-axis --, which has eigenvalue $\lambda_2 = 0.5$.

\begin{equation}
	\begin{pmatrix} 0.5 & 0\\ 0 &  2 \end{pmatrix} \cdot \begin{pmatrix}  1\\ 0 \end{pmatrix} = \begin{pmatrix}  0.5\\ 0 \end{pmatrix}= 0.5 \, \begin{pmatrix}  1\\ 0 \end{pmatrix}
\end{equation}

\begin{marginfigure}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{transform}	
	\end{center}
	\caption{Role of eigenvalues in a matrix transformation. The black eigenvector $\mathbf{v_1} = (0,1)$ has eigenvalue 2 while the red eigenvector $\mathbf{v2}=(1,0)$ has eigenvalue 0.5. When a vector like $\mathbf{u_1}$ -- in blue -- is transformed, it just gets stretched twice in the vertical axis along $v_1$ and compressed to half along the horizontal axis $\mathbf{v_2}$. the result of the transformation $\mathbf{u_2} = \mathbf{A} \, \mathbf{u_1}$ is shown in green}
	\label{fig:transform}
\end{marginfigure}

Figure \ref{fig:transform} shows what happen if we transform any other vector, like $\mathbf{u_1}=\begin{pmatrix}  1\\ 1 \end{pmatrix}$ it will be stretched along the y-axis to be twice as long and shrunk along the x-axis to be half as long:

\begin{equation}
	\begin{pmatrix} 0.5 & 0\\ 0 &  2 \end{pmatrix} \cdot \begin{pmatrix}  1\\ 1 \end{pmatrix} = \begin{pmatrix}  0.5\\ 2 \end{pmatrix}
\end{equation}

Knowing this, we can already predict that multiplying vector $\begin{pmatrix}  2\\ 4 \end{pmatrix}$ will result in vector  $\begin{pmatrix}  1\\ 8 \end{pmatrix}$, so knowing the eigenvectors and the eigenvalues is  knowing how the matrix works.
\section{The eigenvalues of the Leslie matrix}

\subsection{Real eigenvalues}
Leslie matrices tend to have very special eigenvalues. In most cases there is one dominant eigenvalue, larger than one, while every other eigenvalue is less than one. As a result of this, any starting population evolves towards the age structure dictated by the dominant eigenvector after a few generations. Figure \ref{fig:leslie_phase} plots the amount of adult vs senior mice along several generations. We can clearly see how, after only three generations, the numbers move along a straight line, which is the direction of the dominant eigenvector.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{project_n2_n3}	
	\end{center}
	\caption{Progression of the population of adult mice $n_2$ vs senior mice $n_3$. Each point represents the values of these two age groups in one generation and arrows have been added to show the direction in which changes are happening. After just a few generations, the progress goes along a very clear straight line that is determined by the dominant eigenvector of the leslie matrix. }
	\label{fig:leslie_phase}
\end{figure}

It is easy to prove that if a given vector $\mathbf{v}$ is an eigenvector of a matrix, so is any multiple $k \cdot\mathbf{v}$. This demonstration is left as an exercise. For those who feel ready for a bigger challenge, it is also possible to show why the dominant eigenvalue determines the final age structure. \textbf{Hint:} any vector can be written as a linear combination of the eigenvalues.

\subsection{Complex eigenvalues}

In some cases, a Leslie matrix does not behave as stated before. Lets see the following model describing the dynamics of a locust population. Locusts have three stages in their life cycle, just as the example before. The first stage, $n_1$, are the eggs, second stage individuals, $n_2$, are called hoppers since they are not yet able to fly. Finally, when an individual develops proper wings and the ability to fly, it is considered an adult, $n_3$.

The Leslie matrix for this case is:

\begin{equation}
L=\begin{pmatrix}
	0	&0	&1000	\\
	0.02	&0	&0	\\
	0	&0.05	&0	\\
\end{pmatrix}
\end{equation}

Inspection of the Leslie matrix already tells us a few things about the biology of locusts. First of all, the zero in the first row and first column tells us that hoppers are not yet sexually mature $f_2=0$ but adult locusts are extremely prolific with an average production of 1,000 eggs per individual. The survival rate of eggs is extremely low, with the survival rate of hopper being more that twice as high but still very low, with only 5\% of hoppers surviving until the adult stage.

\begin{figure*}
	\begin{center}
		\includegraphics[width=\textwidth]{locusts_evol}
		\includegraphics[width=\textwidth]{tot_locusts_evol}		
	\end{center}
	\caption{The upper graph shows the population structure as a bar graph for nine generations (y axis in log-coordinates). The figure below shows the total population for each generation (the sum of all three age groups). Unlike the previous case, we can clearly see an oscillating population.}
	\label{fig:Llocusts}
\end{figure*}

Simulating nine generations with this matrix, as can be seen in figure \ref{fig:Llocusts} shows a very different behavior. This population does not grow as before, but keeps oscillating instead. Moreover, the age structure does not remain fixed, it keeps switching between three different shapes.


\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{locusts_phaseI}	
	\end{center}
	\caption{Progression of the population of eggs $n_1$ vs hoppers $n_2$. Each point represents the values of these two age groups in one generation and arrows have been added to show the direction in which changes are happening. Only three generations are shown since the third generation ends up exactly in the same position as the first, therefore repeating this cycle over and over }
	\label{fig:leslie_locust_phase}
\end{figure}

We can also view a projection of the amount of eggs and hoppers as we did with mice in figure \ref{fig:leslie_phase}. The result is a completely different behavior shown in figure \ref{fig:leslie_locust_phase}. It is noteworthy that periodic behaviors -- oscillations -- result in closed orbits in the phase space. 



\begin{figure*}
	\begin{center}
		\includegraphics[width=\textwidth]{tot_locusts_evol_dec}
		\includegraphics[width=\textwidth]{tot_locusts_evol_inc}
	\end{center}
	\caption{Evolution of the total number of locusts for different values of adult fecundity. At lower fecundities ($f_3 = 500$) the population exhibits damped oscillations until it eventually reaches extinction $N=0$. For higher fecundities $f_3 = 1,500$, the size of the population increases towards infinity. Obviously, the population will eventually collapse due to lack of resources.}
	\label{fig:Llocusts_stability}
\end{figure*}

Complex eigenvalues always result in oscillatory dynamics, but these oscillations are not always sustained. When the oscillations decrease in amplitude, the oscillation is said to be dampened and the system is said to be stable, because it is going towards a certain point where it will stay as time goes to infinity. If the oscillations have increased amplitude, the system will not be able to sustain the oscillation, and it is said to be unstable.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{locusts_phaseIdec}
		\includegraphics[width=0.45\textwidth]{locusts_phaseIinc}
	\end{center}
	\caption{Phase space representation (only 2D) for two different fecundities. The first three generations are displayed in blue and the next three in black to facilitate interpretation. At low fecundities the orbits are almost closed but every turn is smaller until the system reaches zero as $t \mapsto \infty$. At higher fecundity, however, oscillations become wider.}
	 
	\label{fig:Llocusts_stability_pp}
\end{figure}

\FloatBarrier


\chapter{Continuous dynamical systems}

Not every system can be studied in discrete time jumps. When time becomes a continuous variable, dynamical systems have to be represented using equations that work for any possible time interval. The mathematical tool of choice in these cases are differential equations.


A differential equation is an equality involving one or more unknowns ($x$, $y$, $z$,\dots) and their derivatives. Since our unknowns have derivatives, they are not just numbers like in regular -- a.k.a. algebraic -- equations. Our unknowns are functions or, more accurately, families of functions. We can make this explicit by writing our unknowns as $x(t)$, $y(t)$, $z(t)$,\dots thus sacrificing brevity for the sake of clarity. In the text that follows, very little effort will be made to maintain consistency between these two notations and we will wantonly switch between both. Live with it.

Among all differential equations, we will be interested in the simpler type. Ordinary differential equations (ODEs) are those that do not involve partial derivatives. Our functions will only have one independent variable that almost always will be time. Moreover, we will start with linear equations.

Linear equations are those where different unknowns and their derivatives are only combined linearly. Some examples of linear equations are:

\begin{align}
	\frac{dx}{dt} \: = \: &  a \, x	\nonumber\\
	\frac{dx}{dt} + \frac{dy}{dt}\: = \: &  0 \nonumber\\
	\frac{dx}{dt} \: = \: &  a \, x + b \, y \nonumber
\end{align}

while the following equations are not linear:

\begin{align}
	\frac{dx}{dt} \: = \: &  x^2	\nonumber\\
	\frac{dx}{dt} + \frac{dy}{dt}\: = \: &  x \, y \nonumber\\
	\frac{dx}{dt} \: = \: &  \sin x \nonumber
\end{align}

Keep in mind that the linearity requirement only applies to the unknowns and not to the independent variable. The following equations are all linear


\begin{align}
	\frac{dx}{dt} \: = \: &  t^2 \, x	\nonumber\\
	\frac{dx}{dt} + \frac{dy}{dt}\: = \: &  \sin t \nonumber\\
	\frac{dx}{dt} \: = \: &  t \, x + t^2 \, y \nonumber
\end{align}

In these equations, the coefficients that multiply our unknowns just happen to be functions of time. A linear equation can have coefficients that depend on time and also have time-dependent independent terms. In general, we will not concern ourselves with such systems.

\section{Linear equations}

Solving differential equations is very challenging, that's why we will try to avoid it whenever possible. In fact, we are only going to solve one differential equation during the whole course. We will solve it in two versions: with one variable and with several variables.

\subsection{Solving the linear equation with one variable}

Lets start with the simplest possible case, which also happens to describe a fundamental biological process: exponential growth.

\begin{equation}
	\label{odexp}
	\frac{dx}{dt} = \mu \, x 
\end{equation}

Which can be read as: ``the rate of increase of a population $x$ is proportional to its own size.'' This equation is easy to solve because we can separate the variables:


\begin{equation}
	\frac{dx}{x} = \mu \, dt \nonumber
\end{equation}

and integrate

\begin{equation}
	\int \frac{dx}{x} = \mu \, \int  dt \nonumber
\end{equation}

\begin{equation}
	\ln{x} = \mu \, t + C \nonumber
\end{equation}

Since $C$ is an arbitrary constant, we can define it in terms of another constant $C= \ln{k}$ so:

\begin{equation}
	\ln{x} - \ln{k}  = \mu \, t  \nonumber
\end{equation}

Applying the well known properties of logarithms listed in appendix \ref{logprop}

\begin{equation}
	\ln{\frac{x}{k}}   = \mu \, t \nonumber
\end{equation}

exponentiating both sides and rearranging

\begin{equation}
	x   = k \, e^{\mu \, t} \nonumber
\end{equation}

This is the \emph{general solution} of the differential equation. It is a family of functions because we can generate infinite functions assigning different values to k. For every possible value of $k$ there is a \emph{particular solution} of the equation.

We are not normally interested in general solutions. General solutions are for mathematicians. We want the particular solution that fits some \emph{initial condition} when $t=0$,  often written $x(0)$ or $x_0$. For instance, we have a bacterial culture that starts with an optical density of 0.01. How do we find the right value for k?


\begin{equation}
x(0) = 0.01 \Rightarrow	 k \, e^{\mu \, 0} = 0.01 \Rightarrow	k=0.01 \nonumber
\end{equation}

In general, we use the solution to the exponential equation as:

\begin{equation}
	x   = x_0 \, e^{\mu \, t} \nonumber
\end{equation}

\subsection{Solving the linear equation with several variables}


Our simple linear equation can also have more that one variable:

\begin{align}
	\label{odenvar}
	\frac{dx_1}{dt} =& a_{1,1} \, x_1 + a_{1,2} \, x_2 \nonumber\\
	\frac{dx_2}{dt} =& a_{2,1} \, x_1 + a_{2,2} \, x_2
\end{align}

when dealing with several variables, it helps a lot to use vectors and matrices.

\begin{equation}
	\frac{d}{dt} \begin{pmatrix} x_1\\ x_2 \end{pmatrix} = \begin{pmatrix} a_{1,1} & a_{1,2}\\ a_{2,1} & a_{2,2} \end{pmatrix} \begin{pmatrix} x_1\\ x_2 \end{pmatrix} \nonumber
\end{equation}

Thus, we can write any equation with n variables as:

\begin{equation}
	\label{odenvar_mat}
	\frac{d\mathbf{x}}{dt}  = \mathbf{A} \, \mathbf{x}
\end{equation}

Note that vectors and matrices are written in boldface.

Since the solution of the equation for one variable was an exponential, lets make the hypothesis that an exponential can also be a solution for this case. We assume

\begin{equation}
	\mathbf{x}= \mathbf{v} \, e^{\lambda \, t} \nonumber
\end{equation}

Since $\mathbf{x}$ is a vector now, we multiply our exponential times a  vector of constants $\mathbf{v}$. We can test if this is a valid solution by substituting it into the equation, which gives:

\begin{equation}
	\frac{d}{dt} \left( \mathbf{v} \, e^{\lambda \, t} \right)  = \mathbf{A} \, \mathbf{v} \, e^{\lambda \, t}  \nonumber
\end{equation}

On the left hand side, the vector of  constants can be taken out of the derivative so:

\begin{equation}
	\mathbf{v} \, \frac{d}{dt} \left(  e^{\lambda \, t} \right)  = \mathbf{A} \, \mathbf{v} \, e^{\lambda \, t}  \nonumber
\end{equation}
 
 and
 
 \begin{equation}
 	\mathbf{v}   \lambda \, e^{\lambda \, t}   = \mathbf{A} \, \mathbf{v} \, e^{\lambda \, t} \nonumber
 \end{equation}
 
canceling out the exponentials:

\begin{equation}
\mathbf{A} \, \mathbf{v}  = \lambda  \,	\mathbf{v}   
\end{equation}
 
So $\mathbf{x}=\mathbf{v} \, e^{\lambda \, t}$ will be a solution of the equation as long as $\lambda$ and $\mathbf{v}$ satisfy the equation above, which we had already seen before in equation (\ref{eigendefinition}). In other works, the solution will only be correct if the chosen vector $\mathbf{v}$ is an eigenvector of $\mathbf{A}$ and the exponent $\lambda$ is the corresponding eigenvalue.

We can solve the equation and find all possible values of $\lambda$ by rearranging terms.

\begin{equation}
	\mathbf{A} \, \mathbf{v}  - \mathbf{I} \, \lambda  \,	\mathbf{v}   = 0 \nonumber
\end{equation}

note we have multiplied lambda by the identity matrix $\mathbf{I}$. This is a matrix of zeros everywhere except its main diagonal, which is occupied by ones. The identity matrix plays the same role as number one among scalars, it does not alter the matrices or vectors it multiplies. Now we can factor $\mathbf{v}$ out 

\begin{equation}
	\label{solve4eigenvec}
	\left( \mathbf{A}   - \mathbf{I} \, \lambda     \right) \, \mathbf{v} =  0  
\end{equation}

This is an homogeneous system of equations with the components of $\mathbf{v}$ as unknowns. For the system to have a solution other than the trivial, $\mathbf{v}=\mathbf{0}$, the determinant of the parenthesis must satisfy:

\begin{equation}
	\label{solve4eigenval}
	\left| \mathbf{A}   - \mathbf{I} \, \lambda     \right|  =  0  
\end{equation}

The expansion of this determinant results in a $n-th$ degree equation in terms of lambda known as the characteristic equation of the system.

\begin{equation}
c_n \, \lambda^n +c_{n-1} \, \lambda^{n-1}  + \dots  + c_1 \, \lambda + c_0 = 0 \nonumber
\end{equation}

Solving this equation provides the values of all the eigenvalues of the matrix. By substituting each eigenvalue in equation \ref{solve4eigenvec}, we can obtain the corresponding eigenvectors.

\section{The eigenvalues of a differential equation} 

\subsection{Real eigenvalues}
A matrix $\mathbf{A}$ with size $n \times n$ has n vectors $\mathbf{v_i}$ for $i = 1 \dots n$ called eigenvectors, and each eigenvector has an eigenvalue ($\lambda_i$) such that every pair of eigenvector and eigenvalue satisfy equation (\ref{eigendefinition}).
	
In order to understand the meaning of this and why eigenvector and eigenvalues are extremely important in applied mathematics, we have to think of the matrix product as an operation that transforms one vector into another. For instance, lets take the matrix
\begin{equation}
	\mathbf{A}  = \begin{pmatrix} -1 & -4\\ -3 & -2\end{pmatrix} \nonumber
\end{equation}

If we multiply $(1,2)$ times the matrix we obtain another vector:

\begin{equation}
	 \begin{pmatrix} -1 & -4\\ -3 & -2\end{pmatrix} \,  \begin{pmatrix} 1\\ 2\end{pmatrix}=\begin{pmatrix} -9 \\ -7\end{pmatrix}\nonumber
\end{equation}

So matrix $\mathbf{A}$ transforms  (1,2) into  (-9,-7). Now if  we transform another vector:

\begin{equation}
	\begin{pmatrix} -1 & -4\\ -3 & -2\end{pmatrix} \,  \begin{pmatrix} 4\\ -3\end{pmatrix}=\begin{pmatrix} 8 \\ -6\end{pmatrix}\nonumber
\end{equation}

In the previous case, the transformation changed the direction of our vector but now, vector (4,-3) was transformed into a stretched version of itself with double length. If we now transformed $(8,-6)$ using the matrix, we would obtain $(16,-12)$, which is again the same vector multiplied by two. So (4,-3) is an eigenvector of $\mathbf{A}$ and the corresponding eigenvalue is $\lambda=2$. Note that negative eigenvalues will change the sense of the vector but not its direction, as is the case with the second eigenvector of this matrix $(1,1)$.

\begin{equation}
	\begin{pmatrix} -1 & -4\\ -3 & -2\end{pmatrix} \,  \begin{pmatrix} 1\\ 1\end{pmatrix}=- \begin{pmatrix} 5 \\ 5\end{pmatrix}\nonumber
\end{equation}

Obviously, the corresponding eigenvalue is $\lambda=-5$.

But how can we find out the eigenvectors and eigenvalues of a matrix other than by trial and error? First we get the eigenvalues using equation \ref{solve4eigenval}:

\begin{equation}
  \left|\begin{matrix} -1-\lambda & -4\\ -3 & -2-\lambda\end{matrix} \right| = 0 \nonumber
\end{equation}

we get the characteristic polynomial

\begin{equation}
	\lambda^2  +3 \,  \lambda  - 10 = 0 \nonumber
\end{equation}

A primary school problem we can solve:

\begin{equation}
	\lambda= \frac{-3 \pm \sqrt{9+40}}{2} \nonumber
\end{equation}

with two solutions $\lambda_1=-5$ and $\lambda_2=2$. We have already seen that the eigenvector $\mathbf{v_1}$ for $\lambda_1$ is . Can we find the other one? We can go back to equation (\ref{eigendefinition}) or use the already factored out version in equation  (\ref{solve4eigenvec})

\begin{equation}
\begin{pmatrix} -1-\lambda & -4\\ -3 & -2-\lambda\end{pmatrix} \, \begin{pmatrix} v_x \\  v_y\end{pmatrix} = 0 \nonumber
\end{equation}

For the case $\lambda = 2$:

\begin{equation}
	\begin{pmatrix} -3 & -4\\ -3 & -4\end{pmatrix} \, \begin{pmatrix} v_x \\  v_y\end{pmatrix} = 0 \nonumber
\end{equation}

\begin{align*}
	-3 \, v_x - 4 \, v_y =& 0 \nonumber\\
	-3 \, v_x - 4 \, v_y =& 0 \nonumber
\end{align*}

Which is the same equation twice, so there is no unique solution. This shows a very important property of eigenvectors: they are never unique. It is easy to see that v=(4,-3) cancels out the equation. If we had just chosen $v_x=1$, then our eigenvector would have been $\mathbf{v}=\left( 1,\frac{-3}{4} \right)$.

Now we can do the same for  $\lambda = -5$:

\begin{align*}
	4 \, v_x - 4 \, v_y =& 0 \nonumber\\
	- 3 \, v_x  + 3 \, v_y =& 0 \nonumber
\end{align*}

This is actually the same equation twice. It is easy to see that whenever $v_x=v_y$, both equations will be satisfied, so we can choose $\mathbf{v} = (1, 1)$.

\paragraph{Exercise:} Can you use equation (\ref{eigendefinition}) to show that given an eigenvector $\mathbf{v}$ and a constant $k$, any vector $k \, \mathbf{v}$ will also be an eigenvector?

\subsection{Complex eigenvalues}

Now lets analyze another matrix 

\begin{equation}
	\mathbf{A}  = \begin{pmatrix} 0 & -1\\ 1 & 0\end{pmatrix}  \nonumber
\end{equation}

Characteristic polynomial

\begin{equation}
	\lambda^2 + 1 = 0 \nonumber
\end{equation}

The solutions to this equation are imaginary $\lambda_1 = i$ and $\lambda_1 = -i$.

Complex eigenvalues always come in conjugate pairs: $\lambda_1 = a + b\, i$,  $\lambda_2 = a - b\, i$. As we will see in the next section, the real parts of a complex eigenvalue determine the stability, while the existence of imaginary parts involves the existence of oscillations. Note that imaginary eigenvalues can only appear for systems with two or more variables. This means that for a system to oscillate, it must have at least two variables.


\section{General solutions for linear equations}

So putting together all the eigenvectors of the system and the corresponding eigenvalues, we get the general solution for our equation with n variables:


\begin{equation}
	\label{odenvar_mat_sol}
	\mathbf{x}= c_1 \, \mathbf{v_1} \, e^{\lambda_1 \, t} + c_2 \, \mathbf{v_2} \, e^{\lambda_2 \, t} +  \dots + c_n \, \mathbf{v_n} \, e^{\lambda_n \, t} 
\end{equation}

Where $c_1 \dots c_n$  are arbitrary constants, the ones that can be used to adjust initial values. Each vector $\mathbf{v_i}$ and its corresponding constant $\lambda_i$ are called eigenvector and eigenvalue respectively.
\paragraph{Example:} lets assume our system has eigenvectors $\mathbf{v_1}=\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ and
	$\mathbf{v_2}=\begin{pmatrix} 1 \\ -1 \end{pmatrix}$. If the eigenvalues are $\lambda_1=2$ and $\lambda_2=1$, the general solution will be:
\begin{equation}
	\mathbf{x}= c_1 \, \begin{pmatrix} 1 \\ 1 \end{pmatrix} \, e^{2 \, t} + c_2 \, \begin{pmatrix} 1 \\ -1 \end{pmatrix} \, e^{ t} 
\end{equation}

To get the particular solution for a system with initial value $\mathbf{x_0}=\begin{pmatrix} 0 \\ 2 \end{pmatrix}$, we have to find the values of the arbitrary constants such that for $t=0$:

\begin{equation}
	\begin{pmatrix} 0 \\ 2 \end{pmatrix}= c_1 \, \begin{pmatrix} 1 \\ 1 \end{pmatrix} \, e^{0} + c_2 \, \begin{pmatrix} 1 \\ -1 \end{pmatrix} \, e^{0} 
\end{equation}

which means $c_1=1$,$c_2=-1$. And our particular solution will be:

\begin{equation}
	\mathbf{x(t)}=   \begin{pmatrix} 1 \\ 1 \end{pmatrix} \, e^{2 \, t} +  \begin{pmatrix} -1 \\ 1 \end{pmatrix} \, e^{t} 
\end{equation}

Note that any positive eigenvalue will result in a positive exponential that will keep increasing with time. For this reason, any system with one or more positive eigenvalues, will have some variables increasing to infinity, and will be said to be unstable. On the other hand, in a system where all eigenvalues are negative, all variables will tend to zero, so they will be considered stable.


When the eigenvalues (and eigenvectors) are complex, the solutions described above will be complex. But we are not interested in complex solutions, since all our models have real variables. In fact, as happened with Leslie matrices, complex eigenvalues cause oscillations in the real solutions of the system, lets see a combination of complex exponentials:

\begin{equation}
	x_1= c_1 e^{\left(  a + b\, i\right) \, t} + c_2 \,  e^{\left(  a + b\, i\right)\, t} = 
	c_1  \, e^{a \, t} \, e^{ i \, b \, t} + c_2 \,  e^{a \, t} \, e^{-i \, b \, t} 
	\nonumber
\end{equation}

factoring out the real exponential

\begin{equation}
x_1= e^{a \, t} \,  \left( c_1  \, e^{ i \, b \, t} + c_2 \,  e^{-i \, b \, t}\right) \nonumber
\end{equation}

Using Euler's formula:

\begin{equation}
	\label{euler}
	e^{i \theta} = cos \theta + i \, \sin \theta 
\end{equation}

\begin{equation}
	x_1= e^{a \, t} \,  \left( c_1  \, \left(  \cos{ b \, t} + i \, \sin{ b \, t} \right)+ c_2 \, \left( \cos{- b \, t} + i \, \sin{- b \, t} \right) \right) \nonumber
\end{equation}

\begin{figure}
\begin{center}
	\includegraphics[width=\textwidth]{trigo}
\end{center}
\caption{Some basic trigonometric relations.}
\label{fig:trigo}
\end{figure}

As can be seen in  figure \ref{fig:trigo}, $- \sin{b \, t}=\sin{- b \, t}$ and $\cos{b \, t}=\cos{ - b \, t}$ so we can rearrange terms:

\begin{equation}
	x_1= e^{a \, t} \,  \left( c_1  \, \left(  \cos{ b \, t} + i \, \sin{ b \, t} \right)+ c_2 \, \left( \cos{ b \, t} - i \, \sin{ b \, t} \right) \right) \nonumber
\end{equation}


\begin{equation}
	x_1=   \left( c_1+c_2\right) \, e^{a \, t} \, \cos{ b \, t}  + i \, \,\left( c_1-c_2 \right) \,e^{a \, t} \,   \sin{ b \, t}   \nonumber
\end{equation}

Note that this solution has the form $x_1(t) = u(t) + i \, v(t)$ where both $u(t)$ and $v(t)$ are real functions. Moreover, these functions are linearly independent so a combination of both will be a proper general solution. By defining new arbitrary constants, we will get the general solution that includes al possible real solutions of the equation:

\begin{equation}
	x_1=   e^{a \, t} \,  \left(  A  \, \cos{ b \, t}  +  B  \,   \sin{ b \, t}  \right) \nonumber
\end{equation}

So the real part of our solutions will be the product of an exponential and an oscillatory function. Moreover, the exponential depends only on the real part of the eigenvalue, $\operatorname{Re}(\lambda) = a$, while the oscillations depend on its imaginary part,  $\operatorname{Im}(\lambda) =b$ (see figure \ref{fig:cmplxeigen}). 

\begin{figure}
	\begin{center}
		\includegraphics[width=\textwidth]{complex_eigen}
	\end{center}
	\caption{Overal behavior of solutions when eigenvalues are complex. The sign of $\operatorname{Re}(\lambda)$ determines stability and $\operatorname{Im}(\lambda)$ governs oscillations}
	\label{fig:cmplxeigen}
\end{figure}


\FloatBarrier

\subsection{Differential equations of higher order}

The following differential equation:

\begin{equation}
	 \frac{d^2x}{dt^2} = \frac{k}{m} \, x - g \nonumber
\end{equation}

is called a second order differential equation because it contains a second derivative. This equation can lead to oscillations, even though, we just said only systems with two variables can show this behavior. The reason for that is that any  differential equation of order $n$ can be written as a first order differential equation with n variables. If we add the variable $y(t)$ as the derivative of $x(t)$ to the equation above, it becomes:

\begin{align}
	\frac{dx}{dt} =& \: y \nonumber \\
	\frac{dy}{dt} =& \: \frac{k}{m} \, x - g \nonumber
\end{align}

So any differential equation of order $n$ can be considered a first order differential equation with $n$ variables.

\subsection{Plotting solutions}

The particular solutions of a differential equation can be plotted as each variable being a function of time as shown in figure \ref{fig:cmplxeigen}, but there are alternative representations. A frequently used plot is the phase space like the one shown in figure \ref{fig:phase_plane}. In this plot, the state variables of the system are used as axes. Since time is not explicitly included in the plot, the lines showing the evolution of the system have arrows that indicate the direction in which the system moves as time goes by. In a plot like this, several particular solutions can be shown at the same time, the line representing each solution is called an orbit. 
\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{phase_plane}
	\end{center}
	\caption{Phase plane representation of a particular solution of a two variable oscillatory system.}
	\label{fig:phase_plane}
\end{figure}

\section{Non-linear systems}

Unlike linear equations, non-linear equations rarely have analytic solutions. Fortunately, we can extract a lot of information from a differential equation without solving it. This is so because we are often not interested in every detail about its dynamics but rather on its long term behavior.

\subsection{Steady states}

The first step to analyze non-linear systems is to find out its steady states. for a given non-linear system:

\begin{equation}
	\label{odegeneral}
	\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x})
\end{equation}

we can calculate its steady states as those values $\mathbf{x}= \mathbf{x_0}$ such that:

\begin{equation}
	\label{stst}
	\mathbf{f}(\mathbf{x_0}) = 0
\end{equation}

Steady states are important because once the system reaches it, it will remain in it as long as it is not perturbed.

\paragraph{Example:} Lets take another growth equation:

\begin{equation}
	\label{odgrowth}
	\frac{dx}{dt} = \mu(x) \, x 
\end{equation}

Unlike exponential growth, where $\mu$ is constant, his more general form makes the growth rate dependent on the size of the population. This makes the model more realistic as resources are expected to become scarce once the population density becomes too high. A particular case is that of logistic growth, where $\mu(x) = \mu_{m} (1 - x / K)$. Our equation becomes

\begin{equation}
	\label{logistic}
	\frac{dx}{dt} = \mu_{m} \left(1 - \frac{x}{K} \right) \, x 
\end{equation}

The steady state condition will be:

\begin{equation}
 \mu_{m} \left(1 - \frac{x}{K} \right) \, x = 0
\end{equation}

which can happen in two different ways. Either $x=0$, just like happened for exponential growth, or $x=K$.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{logisticall}
	\end{center}
	\caption{Logistic growth model with $K=25$, the two steady states $x=0$ and $x=25$ are marked with red dashed lines. It can clearly be seen that the system tends to move away from one steady state and towards the other.}
	\label{fig:logisticall}
\end{figure}

\subsection{Stability}

Figure \ref{fig:logisticall} shows how the behavior of the logistic equation is dictated by its two steady states. The system tends to move away from $x=0$, which is therefore called an \emph{unstable steady state} or \emph{repulsor} and towards $x=K$, which is a \emph{stable steady state} or \emph{attractor}. But how do we know whether a steady state is stable?

A steady state is said to be stable when the system tends to return to it after being moved away from it. In other words, lets assume a system has a steady state at $x=x_0$ and we move the system to another, nearby state $x_0+\delta x$. $\delta x$ is a small number and we call it a perturbation. if we define our variable as a composition of the steady state and a time varying perturbation: $x(t) = x_0 + \delta x(t)$, the system can be said to be stable if $\delta x \rightarrow 0$ as $t \rightarrow \infty$. Conversely, we say the system is unstable if $\delta x \rightarrow \infty$ as $t \rightarrow \infty$.  But how can we predict the behavior of $\delta x$? since we can only solve linear systems, we will have to turn this into a linear equation \dots

\subsection{Linearization}

Making the change of variable $x(t) = x_0 + \delta x(t)$ in equation \ref{odegeneral},

\begin{equation}
	\frac{d}{dt} \left( x_0 + \delta x(t) \right) = f(x_0 + \delta x(t)) \nonumber
\end{equation}

The left hand side is the  derivative of a sum and can be rewritten as the sum of derivatives. Moreover, since $\delta x$ is small by definition, we can approximate this as a linear system as shown in equation \ref{TaylorLin}.

\begin{equation}
	 \frac{dx_0}{dt} + \frac{d}{dt}\delta x(t)  = f(x_0) +\left|\frac{df}{dx}\right|_0 \, \delta x(t)) \nonumber
\end{equation}

$x_0$ is constant, so $\frac{dx_0}{dt}=0$ and $f(x_0)=0$ by the definition of steady state. This leaves the system 

\begin{equation}
	\label{svl}
	\frac{d}{dt}\delta x(t)  =  \left|\frac{df}{dx}\right|_{x=x_0} \,  \delta x(t)) 
\end{equation}

Equation \ref{svl} is called variational linearized system and it is easily to solve. If $\left|\frac{df}{dx}\right|_0 < 0$ then $\delta x \rightarrow 0$ and the steady state is stable. If $\left|\frac{df}{dx}\right|_0 > 0$, $\delta x \rightarrow \infty$ and the system is unstable.

\paragraph{Example:} In the logistic equation:

\begin{equation}
\frac{df}{dx} = \mu_{m} \left( 1-\frac{2 x}{K}\right) \nonumber
\end{equation}

 So for the steady state $x=0$, the l.v.s. is:
 
 \begin{equation}
 	\frac{d}{dt}\delta x(t)  = \mu_m \,  \delta x(t))  \nonumber
 \end{equation}
The solution to this equation is a positive exponential so the perturbation $\delta x(t)$ will be amplified with time. This means the steady state is unstable.

and for the steady state $x=K$:

 \begin{equation}
	\frac{d}{dt}\delta x(t)  = -  \mu_m \,  \delta x(t))  \nonumber
\end{equation}Since the solution of this equation is a negative exponential, any perturbation will fade with time and the steady state is stable.

For systems with more variables:

\begin{align}
	\frac{dx}{dt}=&f_x(x,y)  \nonumber\\
	\frac{dy}{dt}=&f_y(x,y)  \nonumber
\end{align}

the l.v.s. looks like this

 \begin{equation}
 	\label{fig:svlnvar}
	\frac{d}{dt} \begin{pmatrix} \delta x\\ \delta y \end{pmatrix}= \begin{pmatrix} \frac{\partial f_x}{\partial x} & \frac{\partial f_x}{\partial y}\\ \frac{\partial f_y}{\partial x} & \frac{\partial f_y}{\partial y} \end{pmatrix} \, \begin{pmatrix} \delta x\\ \delta y \end{pmatrix}
\end{equation}

Again, the Jacobian matrix contains the partial derivatives evaluated at the steady state, so it is a matrix of constants.

\paragraph{Example:} The chemical signaling system in figure \ref{fig:signalpathd} has two components $X$ and $R$ which responds to an input signal $S$:

\begin{align}
	\frac{dR}{dt} =&\: k_1 \, S - k_2 \, X \, R  \nonumber\\
	\frac{dX}{dt} =&\: k_3 \, S - k_4 \, X  \nonumber
\end{align}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.3\textwidth]{signalpathd}
	\end{center}
	\caption{A signaling pathway with input $S$ and output $R$. }
	\label{fig:signalpathd}
\end{figure}

The system has a steady state at:

\begin{align}
	R =&\: \frac{k_1 k_4}{k_2 k_3}  \nonumber\\
	X =&\: \frac{k_3}{k_4}S \nonumber
\end{align}

and the jacobian matrix:

\begin{equation}
\mathbf{J} =	 \begin{pmatrix} - k_2 \, X &  - k_2 \, R \\   0  & -k_4 	\end{pmatrix} \nonumber
\end{equation}

Since we are interested in a particular steady state, we substitute its value in the jacobian matrix to obtain the linearized system:

\begin{equation}
\frac{d}{dt}	 \begin{pmatrix} \delta R \\ \delta S	\end{pmatrix} =	 \begin{pmatrix} - \frac{k_2 k_3}{k_4}S &  - \frac{k_1 k_4}{k_3} \\   0  & -k_4 	\end{pmatrix} \begin{pmatrix} \delta R \\ \delta S	\end{pmatrix} \nonumber
\end{equation}

And we calculate eigenvalues:

\begin{equation}
\left| \begin{matrix} - \frac{k_2 k_3}{k_4}S - \lambda &  - \frac{k_1 k_4}{k_3} \\   0  & -k_4-\lambda 	\end{matrix} \right|=0 \nonumber
\end{equation}

to obtain the characteristic polynomial
\begin{equation}
	\lambda^2 + \left( k_4 + \frac{k_2 k_3}{k_4}S \right) \, \lambda +k_2 k_3 S=0 \nonumber
\end{equation}

The solutions are $\lambda_1 = -k_4$ and $\lambda_2=-\frac{k_2 k_3}{k_4}S$. Since kinetic constants and concentrations are always positive, this system will always be stable.
\paragraph{Example} A chemostat is a microbial culture fed with fresh medium at a constant rate while the fermentation broth flows out of the culture at identical rate to keep constant volume. This system is described by the equations:

\begin{align}
	\frac{dx}{dt}=&\: \left( \mu_M \frac{s}{s+K_s} - D\right) \, x  \nonumber\\
	\frac{ds}{dt}=&\: D\, (s_0-s)-\frac{\mu_M}{Y} \frac{s}{s+K_s}\,x \nonumber
\end{align}
where $D$ is the dilution rate, $\mu_M$ the maximum growth rate and $Y$ the yield in grams of biomass produced per gram of substrate consumed.

This system has two steady states: $x=0$, $s=s_0$ and when the growth rate is equal to the dilution rate:

 \begin{equation}
	D = \mu_M \frac{s}{s+K_s} = \mu(s) \nonumber
\end{equation}

which results in 

\begin{align}
	s=& \frac{D \, K_s}{\mu_M - D} \nonumber \\
	x=& Y \, \left( s_0 - \frac{D \, K_s}{\mu_M - D} \right) \nonumber
\end{align}

The partial derivatives for the Jacobian matrix are:

\begin{align}
	\frac{\partial f_x}{\partial x }=&\:   \mu(s) - D  \nonumber\\
	\frac{\partial f_x}{\partial s }=&\: \mu_M \frac{K_s}{(s+K_s)^2} \, x   \nonumber\\
	\frac{\partial f_s}{\partial x }=&\: - \frac{\mu(s)}{Y}  \nonumber\\
	\frac{\partial f_s}{\partial s }=&\: - D -   \frac{\mu_M}{Y} \frac{K_s}{(s+K_s)^2} \, x  \nonumber
\end{align}

to analyze the stability of the first steady state, we substitute the values $x=0$, $s=s_0$ in the parrtial derivatives:

\begin{align}
	\frac{\partial f_x}{\partial x }=&\:    \mu(s_0) - D  \nonumber\\
	\frac{\partial f_x}{\partial s }=&\: 0  \nonumber\\
	\frac{\partial f_s}{\partial x }=&\:  - \frac{\mu(s_0)}{Y} \nonumber\\
	\frac{\partial f_s}{\partial s }=&\: - D   \nonumber
\end{align}

We can obtain the eigenvalues from:
 \begin{equation}
\left| \begin{matrix}  \mu(s_0) - D - \lambda  & 0\\   - \frac{\mu(s_0)
	}{Y}  &  - D- \lambda
\end{matrix} \right| = 0 \nonumber
\end{equation}

The characteristic polynomial for this case is:

 \begin{equation}
	\lambda^2 + \left( 2 D - \mu(s_0)\right) \lambda  +D^2-D \mu(s_0) = 0  \nonumber
\end{equation}

Resulting in the eigenvalues $\lambda_1 = -2 D$ and $\lambda_2 = 2\,(\mu(s_0) - D)$. Remember that the condition for a steady state to be stable is that all their eigenvalues are negative. So the steady state $x=0$,$s=s_0$ is only stable if $D > \mu(s_0)$. In other words, $\mu(s_0)$ is the highest growth rate the cells can achieve in the chemostat because $s_0$ if the highest concentration of substrate. If the dilution rate is higher than such growth rate, the cells will be washed out until $x=0$, and with no cells to consume it, the substrate concentration in the tank will be the same as in the feed.



\clearpage
\appendix
\section{Taylor Polynomial}
A polynomial can adopt practically any shape. That's why many functions can be approximated by polynomials, at least, within a certain region.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{MMapprox}
	\end{center}
	\caption{Polynomial approximations (dashed lines) to a Michaelis-Menten kinetics (solid blue line). Linear approximation (red) second degree (green) and third degree (orange).}
	\label{fig:MMapprox}
\end{figure}

Figure \ref{fig:MMapprox} shows the following function

\begin{equation}
	V=\frac{S}{2+S} \nonumber
\end{equation}

If we want to approximate this function with a polynomial, we have to start choosing a reference point ($S_0=1$). Whatever approximation we choose will exactly reproduce the function at the reference point. The approximation will lose quality as we move farther from this reference point. The three examples shown in the figure are polynomials of first, second and third degree in red, green and orange respectively. The  equations of the polynomials are:

\begin{align}
	V=& 0.115 + 0.221 \, S\nonumber\\
	V=& -0.032 + 0.515 \, S - 0.147 S^2\nonumber\\
	V=& -0.178 + 0.953 \, S- 0.585 S^2+ 0.146 S^3 \nonumber
\end{align}

For values in the interval $0.75 \leq S \leq 1.25$ all three approximations are very good but as we move away from the reference point, the approximations diverge. In general, higher degree polynomials are more flexible and will have better chances to approximate complicated functions, but this is not always so. In this case, for instance, we can see the second degree polynomial makes a pretty good job for $0.1 \leq S \leq 1.5$ while the third degree polynomial improves the approximation on the higher end but performs worse for $S < 1$.

Since all polynomials have the same structure, finding an approximating polynomial is just a matter of finding the numerical values for each of its coefficients. The Taylor polynomial is one way of finding out the desired coefficients. The concept is very simple. Knowing the value of a function $f(x)$ at the reference point $x_0$ and all the successive derivatives evaluated at that point, the polynomial, at an arbitrary point $x$ will be:

\begin{equation}
	T(x)=f(x_0) + \left|\frac{df}{dx}\right|_0 \left( x - x_0\right)+ \left|\frac{d^2f}{dx^2}\right|_0 \left( x - x_0\right)^2+ \left|\frac{d^3f}{dx^3}\right|_0 \left( x - x_0\right)^3+ \dots
	\nonumber
\end{equation}

where the symbols $\left| \cdot \right|_0$ around the derivatives mean that we are not using the whole derivative, which is a function, but its value at the reference point, which is a number.

The concept is very simple. At the reference point, $x=x_0$ and all terms will cancel out except $f(x_0)$ so the approximation will ve exact. As we move away from the reference point by an increment of x ($\Delta x = x - x_0$) we will first use the derivative to approximate the change in the function $\Delta f= \frac{df}{dx}\Delta x$ and we will refine the calculation by adding terms for each successive derivative.

In the context of this course, we will be interested in approximations very close to the reference state. In these cases, $\left( x - x_0\right) \ll 1$ and higher derivative terms, which are multiplied by higher powers of this number can be considered to be negligible:

\begin{equation}
	\label{TaylorLin}
	T(x) \approx  f(x_0) + \left|\frac{df}{dx}\right|_0 \left( x - x_0\right)
\end{equation}

so we can use the Taylor polynomial to obtain  linear approximations.

Taylor polynomials can easily generalized to more than one variable as long as we remember the concept of partial derivative. When a function has more than one variable $f(x,y)$ we can calculate a partial derivative for each one

\begin{equation}
	\frac{\partial f}{\partial x} =\frac{d}{dx} \left| f(x,y) \right|_{y=const}  \nonumber
\end{equation}

So we just derive as if our function only had one variable, for instance:

\begin{equation}
	f(x,y) = x^2 + x \, y + y^2 \nonumber
\end{equation}


\begin{equation}
	\frac{\partial f}{\partial x} = 2 \, x + y \nonumber
\end{equation}

\begin{equation}
	\frac{\partial f}{\partial y} =  x + 2 \,y \nonumber
\end{equation}

In a system with several variables, the role of the derivative is taken by a matrix of partial derivatives called the Jacobian Matrix $\mathbf{J}$.  $f(x,y)$:

\begin{equation}
	\mathbf{J} = \begin{pmatrix} \frac{\partial f}{\partial x} & \frac{\partial f}{\partial y} \end{pmatrix} \nonumber
\end{equation}

and the linear approximation is:

\begin{equation}
	\label{TaylorLin_nvar}
	\mathbf{T(x)} \approx  \mathbf{f(x_0)} + \mathbf{J} \left( \mathbf{x} - \mathbf{x_0}\right)
\end{equation}

The function in the previous example at the reference point $(x_0, y_0)=(1,2)$ would have $f(x_0, y_0)=7$, so


\begin{equation}
	T(x,y) = 7 + \begin{pmatrix} 4 & 5 \end{pmatrix} \, \begin{pmatrix} 1-x \\ 2-y \end{pmatrix} = 7 +  4- 4 x + 10-5 y  \nonumber
\end{equation}

\begin{equation}
	T(x,y) = 21 - 4 x - 5 y  \nonumber
\end{equation}

\FloatBarrier
%\section{Complex numbers}
%\FloatBarrier

\section{Some properties of logarithms}
\label{logprop}
\begin{equation}
 \log{(a \, b)} =	\log{a} + \log{b} 
\end{equation}

\begin{equation}
	\log{(a / b)} =	\log{a} - \log{b} 
\end{equation}

\begin{equation}
	\log{x^a} =a \, \log{x} 
\end{equation}

\begin{equation}
	e^{\ln{x}} = x 
\end{equation}












\end{document}
